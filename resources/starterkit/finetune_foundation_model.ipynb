{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l4kUv4DT4nU"
      },
      "source": [
        "> *This notebook is a modified version of the Braindecode tuturial [Fine-tuning a Foundation Model (Signal-JEPA)](https://braindecode.org/dev/auto_examples/advanced_training/plot_finetune_foundation_model.html). The main difference is that we use the [2025 EEG challenge](https://eeg2025.github.io/) data.*\n",
        "\n",
        "> **[Note 2025-10-30]** It will soon be even easier to load a pre-trained foundation model from Braindecode, thanks to [PR#795](https://github.com/braindecode/braindecode/pull/795). Many new models will be added!\n",
        "\n",
        "# Fine-tuning a Foundation Model (Signal-JEPA)\n",
        "\n",
        "Foundation models are large-scale pre-trained models that serve as a starting point\n",
        "for a wide range of downstream tasks, leveraging their generalization capabilities.\n",
        "Fine-tuning these models is necessary to adapt them to specific tasks or datasets,\n",
        "ensuring optimal performance in specialized applications.\n",
        "\n",
        "In this tutorial, we demonstrate how to load a pre-trained foundation model\n",
        "and fine-tune it for a specific task. We use the Signal-JEPA model [1]_\n",
        "and a MOABB motor-imagery dataset for this tutorial.\n",
        "   :depth: 2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install braindecode\n",
        "# we need PR#792 which fixes Labram: f69f12b\n",
        "# !pip install git+https://github.com/braindecode/braindecode.git@f69f12b38d33d6341172bdf43457034ccfeab5ba\n",
        "!pip install eegdash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j_satHQfVAxD",
        "outputId": "eaabcb79-2f32-4486-a71b-4ce6f9d872a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: braindecode in /usr/local/lib/python3.12/dist-packages (1.3.0)\n",
            "Requirement already satisfied: mne>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.10.2)\n",
            "Requirement already satisfied: mne_bids>=0.16 in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from braindecode) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from braindecode) (3.15.1)\n",
            "Requirement already satisfied: skorch>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.2.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.5.2)\n",
            "Requirement already satisfied: torchinfo~=1.8 in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.8.0)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (from braindecode) (4.1.2)\n",
            "Requirement already satisfied: linear_attention_transformer in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.19.1)\n",
            "Requirement already satisfied: docstring_inheritance in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.2.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (1.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (2.9.0.post0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.2.0->braindecode) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.2.0->braindecode) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (3.4.0)\n",
            "Requirement already satisfied: axial-positional-embedding in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.3.12)\n",
            "Requirement already satisfied: linformer>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.2.3)\n",
            "Requirement already satisfied: local-attention in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (1.11.2)\n",
            "Requirement already satisfied: product-key-memory>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode) (0.2.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (0.13.1)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (2.32.4)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.10.0->braindecode) (4.5.0)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.12/dist-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode) (0.11.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->braindecode) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch>=1.2.0->braindecode) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from SoundFile>=0.10.0->wfdb->braindecode) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.0->braindecode) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.10.0->braindecode) (3.0.3)\n",
            "Requirement already satisfied: hyper-connections>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from local-attention->linear_attention_transformer->braindecode) (0.2.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb->braindecode) (2.23)\n",
            "Requirement already satisfied: eegdash in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: braindecode>=1.0 in /usr/local/lib/python3.12/dist-packages (from eegdash) (1.3.0)\n",
            "Requirement already satisfied: mne_bids>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.17.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.60.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.12/dist-packages (from eegdash) (4.15.3)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.12/dist-packages (from eegdash) (2025.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from eegdash) (4.67.1)\n",
            "Requirement already satisfied: pymatreader in /usr/local/lib/python3.12/dist-packages (from eegdash) (1.1.0)\n",
            "Requirement already satisfied: eeglabio in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.9.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from eegdash) (13.9.4)\n",
            "Requirement already satisfied: mne>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (3.15.1)\n",
            "Requirement already satisfied: skorch>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.2.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (0.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.5.2)\n",
            "Requirement already satisfied: torchinfo~=1.8 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.8.0)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (4.1.2)\n",
            "Requirement already satisfied: linear_attention_transformer in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (0.19.1)\n",
            "Requirement already satisfied: docstring_inheritance in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (2.2.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->eegdash) (0.43.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.12/dist-packages (from pymatreader->eegdash) (1.0.2)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from pymongo->eegdash) (2.8.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->eegdash) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->eegdash) (2.19.2)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.12/dist-packages (from s3fs->eegdash) (2.25.1)\n",
            "Requirement already satisfied: fsspec==2025.9.0 in /usr/local/lib/python3.12/dist-packages (from s3fs->eegdash) (2025.9.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from s3fs->eegdash) (3.13.1)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (0.12.0)\n",
            "Requirement already satisfied: botocore<1.40.62,>=1.40.46 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.40.61)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (2.9.0.post0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.0.1)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (6.7.0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.8.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->eegdash) (0.1.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (1.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.2.0->braindecode>=1.0->eegdash) (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.4.0)\n",
            "Requirement already satisfied: axial-positional-embedding in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.3.12)\n",
            "Requirement already satisfied: linformer>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.3)\n",
            "Requirement already satisfied: local-attention in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (1.11.2)\n",
            "Requirement already satisfied: product-key-memory>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.11)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->braindecode>=1.0->eegdash) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->braindecode>=1.0->eegdash) (2025.2)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode>=1.0->eegdash) (0.13.1)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode>=1.0->eegdash) (2.32.4)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.40.62,>=1.40.46->aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (2.5.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.10.0->braindecode>=1.0->eegdash) (4.5.0)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.12/dist-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode>=1.0->eegdash) (0.11.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode>=1.0->eegdash) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode>=1.0->eegdash) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode>=1.0->eegdash) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch>=1.2.0->braindecode>=1.0->eegdash) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from SoundFile>=0.10.0->wfdb->braindecode>=1.0->eegdash) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.10.0->braindecode>=1.0->eegdash) (3.0.3)\n",
            "Requirement already satisfied: hyper-connections>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from local-attention->linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb->braindecode>=1.0->eegdash) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stIPF0P8T4nY"
      },
      "outputs": [],
      "source": [
        "# Authors: Pierre Guetschel <pierre.guetschel@gmail.com>\n",
        "#\n",
        "# License: BSD (3-clause)\n",
        "#\n",
        "from pathlib import Path\n",
        "import mne\n",
        "import numpy as np\n",
        "import torch\n",
        "from braindecode import EEGRegressor\n",
        "from braindecode.datasets import MOABBDataset\n",
        "from braindecode.models import SignalJEPA_PreLocal\n",
        "from braindecode.preprocessing import (\n",
        "    create_windows_from_events,\n",
        "    Preprocessor,\n",
        "    preprocess,\n",
        ")\n",
        "from eegdash.dataset import EEGChallengeDataset\n",
        "from eegdash.hbn.windows import (\n",
        "    annotate_trials_with_target,\n",
        "    add_aux_anchors,\n",
        "    add_extras_columns,\n",
        "    keep_only_recordings_with,\n",
        ")\n",
        "\n",
        "torch.manual_seed(12)\n",
        "np.random.seed(12)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    msg ='CUDA-enabled GPU found. Training should be faster.'\n",
        "else:\n",
        "    msg = (\n",
        "        \"No GPU found. Training will be carried out on CPU, which might be \"\n",
        "        \"slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by\"\n",
        "        \" clicking\\n`Runtime/Change runtime type` in the top bar menu, then \"\n",
        "        \"selecting \\'T4 GPU\\'\\nunder \\'Hardware accelerator\\'.\"\n",
        "    )\n",
        "print(msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJFPq_J1Wdvr",
        "outputId": "2f2a445e-bcb9-4031-bd49-936f3ef86e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA-enabled GPU found. Training should be faster.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C5ZITmDT4nY"
      },
      "source": [
        "## Loading and preparing the data\n",
        "\n",
        "### Loading a dataset\n",
        "\n",
        "We start by loading a MOABB dataset, a single subject only for speed.\n",
        "The dataset contains motor imagery EEG recordings, which we will preprocess and use for fine-tuning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "GuPXBTp_T4nY",
        "outputId": "2a9a24f3-1819-4ec0-fc70-52b5153765f3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[33m╭─\u001b[0m\u001b[33m──────────────────────────────────────\u001b[0m\u001b[33m \u001b[0m\u001b[33mEEG 2025 Competition Data Notice\u001b[0m\u001b[33m \u001b[0m\u001b[33m───────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m\n",
              "\u001b[33m│\u001b[0m This object loads the HBN dataset that has been preprocessed for the EEG Challenge:                             \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m   * Downsampled from 500Hz to 100Hz                                                                             \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m   * Bandpass filtered (0.5-50 Hz)                                                                               \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m For full preprocessing applied for competition details, see:                                                    \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m   \u001b]8;id=116485;https://github.com/eeg2025/downsample-datasets\u001b\\https://github.com/eeg2025/downsample-datasets\u001b]8;;\u001b\\                                                                \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m The HBN dataset have some preprocessing applied by the HBN team:                                                \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m   * Re-reference (Cz Channel)                                                                                   \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m                                                                                                                 \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m \u001b[1;31mIMPORTANT\u001b[0m: The data accessed via `EEGChallengeDataset` is \u001b[4mNOT\u001b[0m identical to what you get from \u001b]8;id=73387;https://github.com/sccn/EEGDash/blob/develop/eegdash/api.py\u001b\\EEGDashDataset\u001b]8;;\u001b\\     \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m directly.                                                                                                       \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the    \u001b[33m│\u001b[0m\n",
              "\u001b[33m│\u001b[0m challenge data.                                                                                                 \u001b[33m│\u001b[0m\n",
              "\u001b[33m╰─\u001b[0m\u001b[33m─────────────────────────────────────────\u001b[0m\u001b[33m \u001b[0m\u001b[36mSource: EEGChallengeDataset\u001b[0m\u001b[33m \u001b[0m\u001b[33m─────────────────────────────────────────\u001b[0m\u001b[33m─╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">╭─────────────────────────────────────── EEG 2025 Competition Data Notice ────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> This object loads the HBN dataset that has been preprocessed for the EEG Challenge:                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   * Downsampled from 500Hz to 100Hz                                                                             <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   * Bandpass filtered (0.5-50 Hz)                                                                               <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> For full preprocessing applied for competition details, see:                                                    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   <a href=\"https://github.com/eeg2025/downsample-datasets\" target=\"_blank\">https://github.com/eeg2025/downsample-datasets</a>                                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> The HBN dataset have some preprocessing applied by the HBN team:                                                <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>   * Re-reference (Cz Channel)                                                                                   <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span>                                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">IMPORTANT</span>: The data accessed via `EEGChallengeDataset` is <span style=\"text-decoration: underline\">NOT</span> identical to what you get from <a href=\"https://github.com/sccn/EEGDash/blob/develop/eegdash/api.py\" target=\"_blank\">EEGDashDataset</a>     <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> directly.                                                                                                       <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the    <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">│</span> challenge data.                                                                                                 <span style=\"color: #808000; text-decoration-color: #808000\">│</span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000\">╰────────────────────────────────────────── </span><span style=\"color: #008080; text-decoration-color: #008080\">Source: EEGChallengeDataset</span><span style=\"color: #808000; text-decoration-color: #808000\"> ──────────────────────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "dataset_ccd = EEGChallengeDataset(\n",
        "    task=\"contrastChangeDetection\",\n",
        "    release=\"R1\",\n",
        "    cache_dir=DATA_DIR,\n",
        "    mini=False,\n",
        ")\n",
        "\n",
        "# Set the montage for EEG channel locations\n",
        "montage = mne.channels.make_standard_montage(\"GSN-HydroCel-129\")\n",
        "for ds in dataset_ccd.datasets:\n",
        "    ds.raw.set_montage(montage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktKQ3Mw_T4nZ"
      },
      "source": [
        "### Define Dataset parameters\n",
        "\n",
        "We extract the sampling frequency and ensure that it is consistent across\n",
        "all recordings. We also extract the window size from the annotations and\n",
        "information about the EEG channels (names, positions, etc.).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr58uFodT4nZ",
        "outputId": "5f5c28c8-3676-4223-e7bc-93fa233b4c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SFREQ=100.0, len(chs_info)=129\n"
          ]
        }
      ],
      "source": [
        "# Extract sampling frequency\n",
        "SFREQ = dataset_ccd.datasets[0].raw.info[\"sfreq\"]\n",
        "assert all([ds.raw.info[\"sfreq\"] == SFREQ for ds in dataset_ccd.datasets])\n",
        "\n",
        "# # Extract and validate window size from annotations\n",
        "# window_size_seconds = dataset.datasets[0].raw.annotations.duration[0]\n",
        "# assert all(\n",
        "#     d == window_size_seconds\n",
        "#     for ds in dataset.datasets\n",
        "#     for d in ds.raw.annotations.duration\n",
        "# )\n",
        "\n",
        "# Extract channel information\n",
        "chs_info = dataset_ccd.datasets[0].raw.info[\"chs\"]  # Channel information\n",
        "\n",
        "print(f\"{SFREQ=}, {len(chs_info)=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sroqUpqcT4nZ"
      },
      "source": [
        "### Create Windows from Events\n",
        "\n",
        "We use the `create_windows_from_events` function from Braindecode to segment\n",
        "the dataset into windows based on events.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63QjtxI0T4nZ",
        "outputId": "fed6458b-629d-43fd-90d3-214857cbde1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "   i_window_in_trial  i_start_in_trial  i_stop_in_trial  target  \\\n",
            "0                  0              4278             4478    2.13   \n",
            "1                  0              4798             4998    1.96   \n",
            "2                  0              5478             5678    2.02   \n",
            "3                  0              6318             6518    1.72   \n",
            "4                  0              6838             7038     1.8   \n",
            "5                  0              7358             7558    1.72   \n",
            "6                  0              8038             8238   1.842   \n",
            "7                  0              8719             8919     1.5   \n",
            "8                  0              9399             9599    2.33   \n",
            "9                  0             10919            11119    1.77   \n",
            "\n",
            "   rt_from_stimulus  rt_from_trialstart  stimulus_onset  response_onset  \\\n",
            "0              2.13                4.93          42.284          44.414   \n",
            "1              1.96                4.76          47.484          49.444   \n",
            "2              2.02                6.42          54.284          56.304   \n",
            "3              1.72                7.72          62.684          64.404   \n",
            "4               1.8                 4.6          67.884          69.684   \n",
            "5              1.72                4.52          73.084          74.804   \n",
            "6             1.842                6.24          79.884          81.726   \n",
            "7               1.5                 5.9          86.686          88.186   \n",
            "8              2.33                6.73          93.486          95.816   \n",
            "9              1.77                7.77         108.686         110.456   \n",
            "\n",
            "   correct      response_type  ... thepresent  diaryofawimpykid  \\\n",
            "0        1  right_buttonPress  ...  available         available   \n",
            "1        1  right_buttonPress  ...  available         available   \n",
            "2        1  right_buttonPress  ...  available         available   \n",
            "3        1  right_buttonPress  ...  available         available   \n",
            "4        1   left_buttonPress  ...  available         available   \n",
            "5        1  right_buttonPress  ...  available         available   \n",
            "6        1  right_buttonPress  ...  available         available   \n",
            "7        1   left_buttonPress  ...  available         available   \n",
            "8        1   left_buttonPress  ...  available         available   \n",
            "9        1   left_buttonPress  ...  available         available   \n",
            "\n",
            "  contrastchangedetection_1  contrastchangedetection_2  \\\n",
            "0                 available                  available   \n",
            "1                 available                  available   \n",
            "2                 available                  available   \n",
            "3                 available                  available   \n",
            "4                 available                  available   \n",
            "5                 available                  available   \n",
            "6                 available                  available   \n",
            "7                 available                  available   \n",
            "8                 available                  available   \n",
            "9                 available                  available   \n",
            "\n",
            "  contrastchangedetection_3 surroundsupp_1  surroundsupp_2 seqlearning6target  \\\n",
            "0                 available      available       available        unavailable   \n",
            "1                 available      available       available        unavailable   \n",
            "2                 available      available       available        unavailable   \n",
            "3                 available      available       available        unavailable   \n",
            "4                 available      available       available        unavailable   \n",
            "5                 available      available       available        unavailable   \n",
            "6                 available      available       available        unavailable   \n",
            "7                 available      available       available        unavailable   \n",
            "8                 available      available       available        unavailable   \n",
            "9                 available      available       available        unavailable   \n",
            "\n",
            "  seqlearning8target  symbolsearch  \n",
            "0          available     available  \n",
            "1          available     available  \n",
            "2          available     available  \n",
            "3          available     available  \n",
            "4          available     available  \n",
            "5          available     available  \n",
            "6          available     available  \n",
            "7          available     available  \n",
            "8          available     available  \n",
            "9          available     available  \n",
            "\n",
            "[10 rows x 36 columns]\n"
          ]
        }
      ],
      "source": [
        "EPOCH_LEN_S = 2.0\n",
        "\n",
        "transformation_offline = [\n",
        "    Preprocessor(\n",
        "        annotate_trials_with_target,\n",
        "        target_field=\"rt_from_stimulus\", epoch_length=EPOCH_LEN_S,\n",
        "        require_stimulus=True, require_response=True,\n",
        "        apply_on_array=False,\n",
        "    ),\n",
        "    Preprocessor(add_aux_anchors, apply_on_array=False),\n",
        "]\n",
        "preprocess(dataset_ccd, transformation_offline, n_jobs=3)\n",
        "\n",
        "ANCHOR = \"stimulus_anchor\"\n",
        "SHIFT_AFTER_STIM = 0.5\n",
        "WINDOW_LEN = 2.0\n",
        "\n",
        "# Keep only recordings that actually contain stimulus anchors\n",
        "dataset = keep_only_recordings_with(ANCHOR, dataset_ccd)\n",
        "\n",
        "# Create single-interval windows (stim-locked, long enough to include the response)\n",
        "windows_dataset = create_windows_from_events(\n",
        "    dataset,\n",
        "    mapping={ANCHOR: 0},\n",
        "    trial_start_offset_samples=int(SHIFT_AFTER_STIM * SFREQ),                 # +0.5 s\n",
        "    trial_stop_offset_samples=int((SHIFT_AFTER_STIM + WINDOW_LEN) * SFREQ),   # +2.5 s\n",
        "    window_size_samples=int(EPOCH_LEN_S * SFREQ),\n",
        "    window_stride_samples=int(SFREQ),\n",
        "    preload=True,\n",
        ")\n",
        "\n",
        "# Injecting metadata into the extra mne annotation.\n",
        "windows_dataset = add_extras_columns(\n",
        "    windows_dataset,\n",
        "    dataset,\n",
        "    desc=ANCHOR,\n",
        "    keys=(\"target\", \"rt_from_stimulus\", \"rt_from_trialstart\",\n",
        "          \"stimulus_onset\", \"response_onset\", \"correct\", \"response_type\")\n",
        ")\n",
        "\n",
        "metadata = windows_dataset.get_metadata()\n",
        "print(metadata.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmBTKd3rT4nZ"
      },
      "source": [
        "## Loading a pre-trained foundation model\n",
        "\n",
        "### Download and Load Pre-trained Weights\n",
        "\n",
        "We download the pre-trained weights for the SignalJEPA model from the Hugging Face Hub.\n",
        "These weights will serve as the starting point for finetuning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll8fE7W1T4nZ"
      },
      "outputs": [],
      "source": [
        "model_state_dict = torch.hub.load_state_dict_from_url(\n",
        "    url=\"https://huggingface.co/braindecode/SignalJEPA/resolve/main/signal-jepa_16s-60_adeuwv4s.pth\"\n",
        ")\n",
        "# !wget https://huggingface.co/braindecode/SignalJEPA/resolve/main/signal-jepa_16s-60_adeuwv4s.pth\n",
        "# model_state_dict = torch.load('signal-jepa_16s-60_adeuwv4s.pth')\n",
        "# print(model_state_dict.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4N-Dgd1T4nZ"
      },
      "source": [
        "### Instantiate the Foundation Model\n",
        "\n",
        "We create an instance of the SignalJEPA model using the pre-local downstream\n",
        "architecture. The model is initialized with the dataset's sampling frequency,\n",
        "window size, and channel information.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQx0Y1qvT4nZ"
      },
      "outputs": [],
      "source": [
        "model = SignalJEPA_PreLocal(\n",
        "    sfreq=SFREQ,\n",
        "    input_window_seconds=EPOCH_LEN_S,\n",
        "    chs_info=chs_info,\n",
        "    n_outputs=1,  # Regression task\n",
        ")\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDq9TbtnT4nZ"
      },
      "source": [
        "### Load the Pre-trained Weights into the Model\n",
        "\n",
        "We load the pre-trained weights into the model. The transformer layers are excluded\n",
        "as this module is not used in the pre-local downstream architecture (see [1]_).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yigoQu_2T4na"
      },
      "outputs": [],
      "source": [
        "# Define layers to exclude from the pre-trained weights\n",
        "new_layers = {\n",
        "    \"spatial_conv.1.weight\",\n",
        "    \"spatial_conv.1.bias\",\n",
        "    \"final_layer.1.weight\",\n",
        "    \"final_layer.1.bias\",\n",
        "}\n",
        "\n",
        "# Filter out transformer weights and load the state dictionary\n",
        "model_state_dict = {\n",
        "    k: v for k, v in model_state_dict.items() if not k.startswith(\"transformer.\")\n",
        "}\n",
        "missing_keys, unexpected_keys = model.load_state_dict(model_state_dict, strict=False)\n",
        "\n",
        "# Ensure no unexpected keys and validate missing keys\n",
        "assert unexpected_keys == [], f\"{unexpected_keys=}\"\n",
        "assert set(missing_keys) == new_layers, f\"{missing_keys=}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uC0EWC6T4na"
      },
      "source": [
        "## Fine-tuning the Model\n",
        "\n",
        "Signal-JEPA is a model trained in a self-supervised manner on a masked\n",
        "prediction task. In this task, the model is configured in a many-to-many\n",
        "fashion, which is not suited for a classification task. Therefore, we need to\n",
        "adjust the model architecture for finetuning. This is what is done by the\n",
        ":class:`SignalJEPA_PreLocal`, :class:`SignalJEPA_Contextual`, and\n",
        ":class:`SignalJEPA_PostLocal` classes. In these classes, new layers are added\n",
        "specifically for classification, as described in the article [1]_ and in the following figure:\n",
        "\n",
        "<img src=\"file://_static/model/sjepa_pre-local.jpg\" alt=\"Signal-JEPA Pre-Local Downstream Architecture\" align=\"center\">\n",
        "\n",
        "With this downstream architecture, two options are possible for fine-tuning:\n",
        "\n",
        "1) Fine-tune only the newly added layers\n",
        "2) Fine-tune the entire model\n",
        "\n",
        "### Freezing Pre-trained Layers\n",
        "\n",
        "As the second option is rather straightforward to implement,\n",
        "we will focus on the first option here.\n",
        "We will freeze all layers except the newly added ones.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80aHc8G4T4na",
        "outputId": "fafe4645-0b9f-470b-a9b4-cc8586ccb46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters:\n",
            "spatial_conv.1.weight\n",
            "spatial_conv.1.bias\n",
            "final_layer.1.weight\n",
            "final_layer.1.bias\n",
            "\n",
            "Other modules:\n",
            "{'feature_encoder'}\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name not in new_layers:\n",
        "        param.requires_grad = False\n",
        "\n",
        "print(\"Trainable parameters:\")\n",
        "other_modules = set()\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)\n",
        "    else:\n",
        "        other_modules.add(name.split(\".\")[0])\n",
        "\n",
        "print(\"\\nOther modules:\")\n",
        "print(other_modules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p9B4yeET4na"
      },
      "source": [
        "### Fine-tuning Procedure\n",
        "\n",
        "Finally, we set up the fine-tuning procedure using Braindecode's\n",
        ":class:`EEGRegressor`. We define the loss function, optimizer, and training\n",
        "parameters. We then fit the model to the windows dataset.\n",
        "\n",
        "We only train for a few epochs for demonstration purposes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "ivLIXT-GT4na",
        "outputId": "3e499176-e9ad-4719-fa15-1b0d3f51626e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[10/30/25 11:04:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m The module passed is already initialized which is not recommended. \u001b]8;id=734463;file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py\u001b\\\u001b[2meegneuralnet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=102984;file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py#184\u001b\\\u001b[2m184\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         Instead, you can pass the module class and its parameters          \u001b[2m                   \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         separately.                                                        \u001b[2m                   \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         For more details, see                                              \u001b[2m                   \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://skorch.readthedocs.io/en/stable/user/neuralnet.html#module\u001b[0m \u001b[2m                   \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         Skipping setting signal-related parameters from data.              \u001b[2m                   \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/30/25 11:04:19] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> The module passed is already initialized which is not recommended. <a href=\"file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">eegneuralnet.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py#184\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Instead, you can pass the module class and its parameters          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         separately.                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         For more details, see                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://skorch.readthedocs.io/en/stable/user/neuralnet.html#module</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Skipping setting signal-related parameters from data.              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    train_neg_root_mean_squared_error    valid_loss    valid_neg_root_mean_squared_error     dur\n",
            "-------  ------------  -----------------------------------  ------------  -----------------------------------  ------\n",
            "      1        \u001b[36m2.5882\u001b[0m                              \u001b[32m-1.5611\u001b[0m        \u001b[35m2.4270\u001b[0m                              \u001b[31m-1.5579\u001b[0m  5.2947\n",
            "      2        \u001b[36m2.3084\u001b[0m                              \u001b[32m-1.4675\u001b[0m        \u001b[35m2.1564\u001b[0m                              \u001b[31m-1.4685\u001b[0m  4.3558\n",
            "      3        \u001b[36m2.0284\u001b[0m                              \u001b[32m-1.3733\u001b[0m        \u001b[35m1.9051\u001b[0m                              \u001b[31m-1.3803\u001b[0m  3.5982\n",
            "      4        \u001b[36m1.7724\u001b[0m                              \u001b[32m-1.2801\u001b[0m        \u001b[35m1.6724\u001b[0m                              \u001b[31m-1.2932\u001b[0m  4.3484\n",
            "      5        \u001b[36m1.5264\u001b[0m                              \u001b[32m-1.1890\u001b[0m        \u001b[35m1.4581\u001b[0m                              \u001b[31m-1.2075\u001b[0m  3.5314\n",
            "      6        \u001b[36m1.3190\u001b[0m                              \u001b[32m-1.1015\u001b[0m        \u001b[35m1.2636\u001b[0m                              \u001b[31m-1.1241\u001b[0m  3.8307\n",
            "      7        \u001b[36m1.1252\u001b[0m                              \u001b[32m-1.0198\u001b[0m        \u001b[35m1.0915\u001b[0m                              \u001b[31m-1.0447\u001b[0m  3.5959\n",
            "      8        \u001b[36m0.9712\u001b[0m                              \u001b[32m-0.9443\u001b[0m        \u001b[35m0.9396\u001b[0m                              \u001b[31m-0.9693\u001b[0m  3.5578\n",
            "      9        \u001b[36m0.8321\u001b[0m                              \u001b[32m-0.8769\u001b[0m        \u001b[35m0.8108\u001b[0m                              \u001b[31m-0.9005\u001b[0m  5.6833\n",
            "     10        \u001b[36m0.7148\u001b[0m                              \u001b[32m-0.8168\u001b[0m        \u001b[35m0.6995\u001b[0m                              \u001b[31m-0.8364\u001b[0m  3.5919\n"
          ]
        }
      ],
      "source": [
        "clf = EEGRegressor(\n",
        "    model,\n",
        "    optimizer=torch.optim.AdamW,\n",
        "    optimizer__lr=0.005,\n",
        "    batch_size=64,\n",
        "    callbacks=['neg_root_mean_squared_error'],\n",
        "    device=device,\n",
        ")\n",
        "_ = clf.fit(windows_dataset, y=metadata[\"target\"], epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIT4oQqiT4na"
      },
      "source": [
        "### All-in-one Implementation\n",
        "\n",
        "In the implementation above, we manually loaded the weights and froze the layers.\n",
        "This forces us to pass an initialized model to :class:`EEGRegressor`, which may\n",
        "create issues if we use it in a cross-validation setting.\n",
        "\n",
        "Instead, we can implement the same procedure in a more compact and reproducible way,\n",
        "by using skorch's callback system.\n",
        "\n",
        "Here, we import a callback to freeze layers and define a custom\n",
        "callback to load the pre-trained weights at the beginning of training:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IU2QtBRT4nb"
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import Callback, Freezer\n",
        "\n",
        "\n",
        "class WeightsLoader(Callback):\n",
        "    def __init__(self, url, strict=False):\n",
        "        self.url = url\n",
        "        self.strict = strict\n",
        "\n",
        "    def on_train_begin(self, net, X=None, y=None, **kwargs):\n",
        "        state_dict = torch.hub.load_state_dict_from_url(url=self.url)\n",
        "        net.module_.load_state_dict(state_dict, strict=self.strict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iUqbTMRT4nb"
      },
      "source": [
        "We can now define a classifier with those callbacks, without having\n",
        "to pass an initialized model, and fit it as before:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgS-DwrNT4nb"
      },
      "outputs": [],
      "source": [
        "classifiers = {}\n",
        "\n",
        "classifiers['SignalJEPA_PreLocal'] = EEGRegressor(\n",
        "    \"SignalJEPA_PreLocal\",\n",
        "    optimizer=torch.optim.AdamW,\n",
        "    optimizer__lr=0.005,\n",
        "    batch_size=64,\n",
        "    callbacks=[\n",
        "        'neg_root_mean_squared_error',\n",
        "        WeightsLoader(\n",
        "            url=\"https://huggingface.co/braindecode/SignalJEPA/resolve/main/signal-jepa_16s-60_adeuwv4s.pth\"\n",
        "        ),\n",
        "        Freezer(patterns=\"feature_encoder.*\"),\n",
        "    ],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in classifiers.items():\n",
        "    _ = clf.fit(windows_dataset, y=metadata[\"target\"], epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "RteyxJtSd2cP",
        "outputId": "f2a5e44a-e8e5-4373-923c-09979305ebdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[10/30/25 11:05:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using Dataset \u001b[1m<\u001b[0m\u001b[1;95mbraindecode.datasets.base.BaseConcatDataset\u001b[0m\u001b[39m object \u001b[0m \u001b]8;id=126596;file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py\u001b\\\u001b[2meegneuralnet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=959112;file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py#220\u001b\\\u001b[2m220\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[39mat \u001b[0m\u001b[1;36m0x7a5e0d58c6b0\u001b[0m\u001b[1m>\u001b[0m to find signal-related parameters.              \u001b[2m                   \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/30/25 11:05:31] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Using Dataset <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">braindecode.datasets.base.BaseConcatDataset</span><span style=\"color: #000000; text-decoration-color: #000000\"> object </span> <a href=\"file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">eegneuralnet.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py#220\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #000000; text-decoration-color: #000000\">at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7a5e0d58c6b0</span><span style=\"font-weight: bold\">&gt;</span> to find signal-related parameters.              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[10/30/25 11:05:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Passing additional parameters \u001b[1m{\u001b[0m\u001b[32m'n_times'\u001b[0m: \u001b[1;36m200\u001b[0m, \u001b[32m'n_chans'\u001b[0m: \u001b[1;36m129\u001b[0m,     \u001b]8;id=920302;file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py\u001b\\\u001b[2meegneuralnet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146360;file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py#253\u001b\\\u001b[2m253\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32m'n_outputs'\u001b[0m: \u001b[1;36m1\u001b[0m\u001b[1m}\u001b[0m to module \u001b[32m'SignalJEPA_PreLocal'\u001b[0m.                   \u001b[2m                   \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/30/25 11:05:32] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Passing additional parameters <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'n_times'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'n_chans'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">129</span>,     <a href=\"file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">eegneuralnet.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.12/dist-packages/braindecode/eegneuralnet.py#253\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">253</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'n_outputs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">}</span> to module <span style=\"color: #008000; text-decoration-color: #008000\">'SignalJEPA_PreLocal'</span>.                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss    train_neg_root_mean_squared_error    valid_loss    valid_neg_root_mean_squared_error     dur\n",
            "-------  ------------  -----------------------------------  ------------  -----------------------------------  ------\n",
            "      1        \u001b[36m2.6664\u001b[0m                              \u001b[32m-1.5843\u001b[0m        \u001b[35m2.5012\u001b[0m                              \u001b[31m-1.5815\u001b[0m  4.9509\n",
            "      2        \u001b[36m2.3731\u001b[0m                              \u001b[32m-1.4917\u001b[0m        \u001b[35m2.2270\u001b[0m                              \u001b[31m-1.4923\u001b[0m  4.8098\n",
            "      3        \u001b[36m2.1096\u001b[0m                              \u001b[32m-1.3981\u001b[0m        \u001b[35m1.9713\u001b[0m                              \u001b[31m-1.4040\u001b[0m  4.2959\n",
            "      4        \u001b[36m1.8351\u001b[0m                              \u001b[32m-1.3043\u001b[0m        \u001b[35m1.7328\u001b[0m                              \u001b[31m-1.3164\u001b[0m  5.1132\n",
            "      5        \u001b[36m1.5817\u001b[0m                              \u001b[32m-1.2122\u001b[0m        \u001b[35m1.5136\u001b[0m                              \u001b[31m-1.2303\u001b[0m  4.3663\n",
            "      6        \u001b[36m1.3617\u001b[0m                              \u001b[32m-1.1225\u001b[0m        \u001b[35m1.3119\u001b[0m                              \u001b[31m-1.1454\u001b[0m  4.3404\n",
            "      7        \u001b[36m1.1736\u001b[0m                              \u001b[32m-1.0377\u001b[0m        \u001b[35m1.1309\u001b[0m                              \u001b[31m-1.0634\u001b[0m  5.0234\n",
            "      8        \u001b[36m0.9995\u001b[0m                              \u001b[32m-0.9589\u001b[0m        \u001b[35m0.9715\u001b[0m                              \u001b[31m-0.9856\u001b[0m  4.3054\n",
            "      9        \u001b[36m0.8603\u001b[0m                              \u001b[32m-0.8863\u001b[0m        \u001b[35m0.8303\u001b[0m                              \u001b[31m-0.9112\u001b[0m  4.3672\n",
            "     10        \u001b[36m0.7297\u001b[0m                              \u001b[32m-0.8219\u001b[0m        \u001b[35m0.7099\u001b[0m                              \u001b[31m-0.8426\u001b[0m  5.0683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9KQQKeuT4nb"
      },
      "source": [
        "## Conclusion and Next Steps\n",
        "\n",
        "In this tutorial, we demonstrated how to fine-tune a pre-trained foundation\n",
        "model, Signal-JEPA, for a motor imagery classification task. We now have a basic\n",
        "implementation that can automatically load pre-trained weights and freeze specific layers.\n",
        "\n",
        "This setup can easily be extended to explore different fine-tuning techniques,\n",
        "base foundation models, and downstream tasks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJzmlnPvT4nb"
      },
      "source": [
        "## References\n",
        "\n",
        ".. [1] Guetschel, P., Moreau, T., and Tangermann, M. (2024)\n",
        "       “S-JEPA: towards seamless cross-dataset transfer\n",
        "       through dynamic spatial attention”.  https://arxiv.org/abs/2403.11772\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Braindecode model in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datasets import MOABBDataset\n",
    "\n",
    "subject_id = 3\n",
    "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[subject_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rocioperez/Documents/Projects/AFG/Brainy/.venv/lib/python3.9/site-packages/braindecode/preprocessing/preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Filtering raw data in 1 contiguous segment\n",
      "\n",
      "FIR filter parameters\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Lower passband edge: 4.00\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "\n",
      "- Lower passband edge: 4.00\n",
      "- Upper passband edge: 38.00 Hz\n",
      "FIR filter parameters\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "---------------------\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "Filtering raw data in 1 contiguous segment\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 38 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 38.00 Hz\n",
      "- Upper transition bandwidth: 9.50 Hz (-6 dB cutoff frequency: 42.75 Hz)\n",
      "- Filter length: 413 samples (1.652 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<braindecode.datasets.moabb.MOABBDataset at 0x106e79610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    "    Preprocessor,\n",
    ")\n",
    "\n",
    "low_cut_hz = 4.0  # low cut frequency for filtering\n",
    "high_cut_hz = 38.0  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "transforms = [\n",
    "    Preprocessor(\"pick_types\", eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "    Preprocessor(\n",
    "        lambda data, factor: np.multiply(data, factor),  # Convert from V to uV\n",
    "        factor=1e6,\n",
    "    ),\n",
    "    Preprocessor(\"filter\", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
    "    Preprocessor(\n",
    "        exponential_moving_standardize,  # Exponential moving standardization\n",
    "        factor_new=factor_new,\n",
    "        init_block_size=init_block_size,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Transform the data\n",
    "preprocess(dataset, transforms, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut Compute Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "from braindecode.preprocessing import create_windows_from_events\n",
    "\n",
    "trial_start_offset_seconds = -0.5\n",
    "# Extract sampling frequency, check that they are same in all datasets\n",
    "sfreq = dataset.datasets[0].raw.info[\"sfreq\"]\n",
    "assert all([ds.raw.info[\"sfreq\"] == sfreq for ds in dataset.datasets])\n",
    "# Calculate the trial start offset in samples.\n",
    "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "windows_dataset = create_windows_from_events(\n",
    "    dataset,\n",
    "    trial_start_offset_samples=trial_start_offset_samples,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "============================================================================================================================================\n",
      "ShallowFBCSPNet (ShallowFBCSPNet)        [1, 22, 1125]             [1, 4]                    --                        --\n",
      "â”œâ”€Ensure4d (ensuredims): 1-1             [1, 22, 1125]             [1, 22, 1125, 1]          --                        --\n",
      "â”œâ”€Rearrange (dimshuffle): 1-2            [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --\n",
      "â”œâ”€CombinedConv (conv_time_spat): 1-3     [1, 1, 1125, 22]          [1, 40, 1101, 1]          36,240                    --\n",
      "â”œâ”€BatchNorm2d (bnorm): 1-4               [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --\n",
      "â”œâ”€Expression (conv_nonlin_exp): 1-5      [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --\n",
      "â”œâ”€AvgPool2d (pool): 1-6                  [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]\n",
      "â”œâ”€Expression (pool_nonlin_exp): 1-7      [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
      "â”œâ”€Dropout (drop): 1-8                    [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
      "â”œâ”€Sequential (final_layer): 1-9          [1, 40, 69, 1]            [1, 4]                    --                        --\n",
      "â”‚    â””â”€Conv2d (conv_classifier): 2-1     [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]\n",
      "â”‚    â””â”€LogSoftmax (logsoftmax): 2-2      [1, 4, 1, 1]              [1, 4, 1, 1]              --                        --\n",
      "â”‚    â””â”€Expression (squeeze): 2-3         [1, 4, 1, 1]              [1, 4]                    --                        --\n",
      "============================================================================================================================================\n",
      "Total params: 47,364\n",
      "Trainable params: 47,364\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.01\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 0.35\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.50\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rocioperez/Documents/Projects/AFG/Brainy/.venv/lib/python3.9/site-packages/braindecode/models/base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "/Users/rocioperez/Documents/Projects/AFG/Brainy/.venv/lib/python3.9/site-packages/braindecode/models/base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = \"cuda\" if cuda else \"cpu\"\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes = 4\n",
    "classes = list(range(n_classes))\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_channels = windows_dataset[0][0].shape[0]\n",
    "input_window_samples = windows_dataset[0][0].shape[1]\n",
    "\n",
    "# The ShallowFBCSPNet is a `nn.Sequential` model\n",
    "\n",
    "model = ShallowFBCSPNet(\n",
    "    n_channels,\n",
    "    n_classes,\n",
    "    input_window_samples=input_window_samples,\n",
    "    final_conv_length=\"auto\",\n",
    ")\n",
    "\n",
    "# Display torchinfo table describing the model\n",
    "print(model)\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily split the dataset using additional info stored in the description attribute, in this case the session column. We select Train for training and test for testing. For other datasets, you might have to choose another column.\n",
    "\n",
    "Note\n",
    "\n",
    "No matter which of the three schemes you use, this initial two-fold split into train_set and test_set always remains the same. Remember that you are not allowed to use the test_set during any stage of training or tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = windows_dataset.split(\"session\")\n",
    "train_set = splitted['0train']  # Session train\n",
    "test_set = splitted['1test']  # Session evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Pure PyTorch training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "lr = 0.0625 * 0.01\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method runs one training epoch over the dataloader for the given model. It needs a loss function, optimization algorithm, and learning rate updating callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Define a method for training one epoch\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "        dataloader: DataLoader, model: Module, loss_fn, optimizer,\n",
    "        scheduler: LRScheduler, epoch: int, device, print_batch_stats=True\n",
    "):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader),\n",
    "                        disable=not print_batch_stats)\n",
    "\n",
    "    for batch_idx, (X, y, _) in progress_bar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  # update the model weights\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "        if print_batch_stats:\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch}/{n_epochs}, \"\n",
    "                f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n",
    "                f\"Loss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    correct /= len(dataloader.dataset)\n",
    "    return train_loss / len(dataloader), correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similarly, the evaluation function loops over the entire dataloader and accumulate the metrics, but doesnâ€™t update the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Batch 5/5, Loss: 1.724403: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 15.62it/s]\n",
      "Batch 5/5, Loss: 6.813942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 31.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 25.0%, Test Loss: 6.379408\n",
      "\n",
      "Train Accuracy: 24.65%, Average Train Loss: 1.652134, Test Accuracy: 25.0%, Average Test Loss: 6.379408\n",
      "\n",
      "Epoch 2/2: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Batch 5/5, Loss: 1.063915: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 16.70it/s]\n",
      "Batch 5/5, Loss: 5.639775: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 35.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 25.0%, Test Loss: 5.266672\n",
      "\n",
      "Train Accuracy: 39.58%, Average Train Loss: 1.261040, Test Accuracy: 25.0%, Average Test Loss: 5.266672\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def test_model(\n",
    "    dataloader: DataLoader, model: Module, loss_fn, print_batch_stats=True\n",
    "):\n",
    "    size = len(dataloader.dataset)\n",
    "    n_batches = len(dataloader)\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    if print_batch_stats:\n",
    "        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    else:\n",
    "        progress_bar = enumerate(dataloader)\n",
    "\n",
    "    for batch_idx, (X, y, _) in progress_bar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        batch_loss = loss_fn(pred, y).item()\n",
    "\n",
    "        test_loss += batch_loss\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        if print_batch_stats:\n",
    "            progress_bar.set_description(\n",
    "                f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n",
    "                f\"Loss: {batch_loss:.6f}\"\n",
    "            )\n",
    "\n",
    "    test_loss /= n_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(\n",
    "        f\"Test Accuracy: {100 * correct:.1f}%, Test Loss: {test_loss:.6f}\\n\"\n",
    "    )\n",
    "    return test_loss, correct\n",
    "\n",
    "\n",
    "# Define the optimization\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                       T_max=n_epochs - 1)\n",
    "# Define the loss function\n",
    "# We used the NNLoss function, which expects log probabilities as input\n",
    "# (which is the case for our model output)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "# train_set and test_set are instances of torch Datasets, and can seamlessly be\n",
    "# wrapped in data loaders.\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{n_epochs}: \", end=\"\")\n",
    "\n",
    "    train_loss, train_accuracy = train_one_epoch(\n",
    "        train_loader, model, loss_fn, optimizer, scheduler, epoch, device,\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy = test_model(test_loader, model, loss_fn)\n",
    "\n",
    "    print(\n",
    "        f\"Train Accuracy: {100 * train_accuracy:.2f}%, \"\n",
    "        f\"Average Train Loss: {train_loss:.6f}, \"\n",
    "        f\"Test Accuracy: {100 * test_accuracy:.1f}%, \"\n",
    "        f\"Average Test Loss: {test_loss:.6f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Train it with PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/rocioperez/Documents/Projects/AFG/Brainy/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name   | Type            | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | module | ShallowFBCSPNet | 47.4 K | eval \n",
      "1 | loss   | NLLLoss         | 0      | train\n",
      "---------------------------------------------------\n",
      "47.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "47.4 K    Total params\n",
      "0.189     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "15        Modules in eval mode\n",
      "/Users/rocioperez/Documents/Projects/AFG/Brainy/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n",
      "/Users/rocioperez/Documents/Projects/AFG/Brainy/.venv/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.70it/s, v_num=0] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.71it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rocioperez/Documents/Projects/AFG/Brainy/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:149: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at /Users/rocioperez/Documents/Projects/AFG/Brainy/notebooks/braindecode-tutorials/lightning_logs/version_0/checkpoints/epoch=1-step=10.ckpt\n",
      "Loaded model weights from the checkpoint at /Users/rocioperez/Documents/Projects/AFG/Brainy/notebooks/braindecode-tutorials/lightning_logs/version_0/checkpoints/epoch=1-step=10.ckpt\n",
      "/Users/rocioperez/Documents/Projects/AFG/Brainy/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.04it/s]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "        test_acc            0.2951388955116272\n",
      "        test_loss           2.8892276287078857\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.2951388955116272, 'test_loss': 2.8892276287078857}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightning as L\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "\n",
    "class LitModule(L.LightningModule):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.loss = torch.nn.NLLLoss()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self.module(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self.module(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = accuracy(y_hat, y, \"multiclass\", num_classes=4)\n",
    "        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n",
    "                                      weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                               T_max=n_epochs - 1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "# Creating the trainer with max_epochs=2 for demonstration purposes\n",
    "trainer = L.Trainer(max_epochs=n_epochs)\n",
    "# Create and train the LightningModule\n",
    "lit_model = LitModule(model)\n",
    "trainer.fit(lit_model, train_loader)\n",
    "\n",
    "# After training, you can test the model using the test DataLoader\n",
    "trainer.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
